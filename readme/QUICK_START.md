# Quick Start - Multi-Model Experiments

## ğŸ¯ Goal
Run multiple gesture recognition experiments with different model sizes to find the best performing architecture and avoid overfitting.

## ğŸ“‹ What's Available

6 model architectures ranging from 150K to 51M parameters:

```
tiny (150K)    â†’ Quick experiments
small (1.1M)   â†’ Simple datasets  
medium (2.5M)  â†’ Balanced (recommended)
large (14M)    â†’ High capacity
xlarge (24M)   â†’ Very high capacity
xxlarge (51M)  â†’ Extreme capacity
```

## ğŸš€ Quick Commands

### Run Single Experiment

```bash
cd /home/nishant/project/irds/model/train

# Medium model (recommended)
python train_pytorch_only.py --config ../../config/experiment_medium.yaml

# Or try different sizes
python train_pytorch_only.py --config ../../config/experiment_tiny.yaml
python train_pytorch_only.py --config ../../config/experiment_small.yaml
python train_pytorch_only.py --config ../../config/experiment_large.yaml
```

### Run All Experiments

```bash
cd /home/nishant/project/irds/model/train
./run_experiments.sh
```

### Compare Results

```bash
cd /home/nishant/project/irds/model/train
python compare_experiments.py
```

Output will show:
- Accuracy comparison table
- Train vs test performance
- Overfitting analysis
- Best model recommendation

## ğŸ–¥ï¸ HPC Commands

```bash
# On HPC, set GPU first
export HIP_VISIBLE_DEVICES=1

# Then run experiments
cd /path/to/irds/model/train
./run_experiments.sh

# Compare results
python compare_experiments.py
```

## ğŸ“ Output Files

Results saved in `model/train/outputs/`:

```
outputs/
â”œâ”€â”€ model_tiny.pth          # Model weights
â”œâ”€â”€ scaler_tiny.json        # Feature scaler
â”œâ”€â”€ info_tiny.json          # Training metrics
â”œâ”€â”€ curves_tiny.png         # Training plots
â”œâ”€â”€ confusion_tiny.png      # Confusion matrix
â”œâ”€â”€ model_small.pth
â”œâ”€â”€ ...
```

## âš™ï¸ Customizing Experiments

Edit config files in `/home/nishant/project/irds/config/`:

```yaml
# config/experiment_medium.yaml
model:
  architecture: "medium"    # Change to: tiny, small, large, etc.

training:
  num_epochs: 150          # Adjust as needed
  batch_size: 16
  learning_rate: 0.0003

data:
  max_files: null          # null = use all files
  sequence_length: 20
```

## ğŸ” Interpreting Results

After running `compare_experiments.py`, look for:

1. **Best Test Accuracy**: Which model performs best on unseen data
2. **Overfitting**: Gap between train and test accuracy
   - < 5% gap: âœ“ Good
   - 5-10%: âš  Slight overfit
   - 10-20%: âš âš  Moderate overfit  
   - > 20%: âœ—âœ— Severe overfit

3. **If Overfitting**:
   - Use a smaller model
   - Increase dropout in config
   - Get more training data

## ğŸ“Š Example Workflow

```bash
# 1. Run all experiments
cd /home/nishant/project/irds/model/train
./run_experiments.sh

# 2. Compare results
python compare_experiments.py

# 3. Based on results, fine-tune best model by editing its config
# Edit config/experiment_<best_model>.yaml

# 4. Re-run best model
python train_pytorch_only.py --config ../../config/experiment_<best_model>.yaml

# 5. Check results
python compare_experiments.py
```

## âœ… Testing

Verify everything works:

```bash
cd /home/nishant/project/irds/model/train

# Quick test (should complete in a few seconds)
python -c "
from pathlib import Path
import sys
sys.path.insert(0, str(Path.cwd().parent.parent))
from model.model_architectures import get_model, MODEL_INFO
print('Available models:', list(MODEL_INFO.keys()))
import torch
model = get_model('tiny', 75, 10, 'cpu')
print('âœ“ Test passed!')
"
```

## ğŸ“– More Information

- **Detailed docs**: `model/train/README_EXPERIMENTS.md`
- **Setup summary**: `MULTI_MODEL_SETUP.md`
- **Model code**: `model/model_architectures.py`

## ğŸ†˜ Troubleshooting

**Import errors?**
```bash
# Check __init__.py exists
ls /home/nishant/project/irds/__init__.py
```

**GPU errors on HPC?**
- Set `HIP_VISIBLE_DEVICES` before running
- GPU warm-up code is already included in the script to prevent segfaults

**Out of memory?**
- Reduce `batch_size` in config
- Use a smaller model architecture
- Reduce `sequence_length` in config

## ğŸ“ Tips

1. **Start with medium model** - Good balance of capacity and speed
2. **Run all sizes once** - Understand the capacity/performance tradeoff
3. **Watch for overfitting** - Large models may memorize training data
4. **Compare systematically** - Use `compare_experiments.py` after each run
5. **Track your experiments** - Output files named by model size for easy comparison

