# High Accuracy Configuration (Target: 90%+)
# More data, longer training, smaller learning rate

debug:
  level: "Info"  # Options: "Debug", "Info", "Warning", "Error", "Off"
  show_tensor_info: true
  show_memory_usage: true
  show_timing: true

data:
  folder_path: null  # Will use project data directory
  max_files: null                   # Use all available files
  sequence_length: 20               # Longer sequences for better context
  test_size: 0.15                   # Smaller test set for more training data
  random_state: 42

model:
  architecture: "medium"           # Options: tiny, small, medium, large, xlarge, xxlarge
  # Model size guide:
  #   tiny: ~500K params - fast experiments
  #   small: ~1.5M params - simple datasets
  #   medium: ~4M params - balanced (recommended)
  #   large: ~11M params - high capacity
  #   xlarge: ~20M params - may overfit
  #   xxlarge: ~35M params - very large datasets only

training:
  num_epochs: 150                  # More epochs
  batch_size: 8                    # Smaller batch size for better learning
  learning_rate: 0.0001           # Lower learning rate for stability
  
  optimizer: "AdamW"
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8
  
  lr_scheduler: "CosineAnnealingLR"
  lr_warmup_epochs: 20
  lr_min: 0.000001
  
  contrastive_weight: 0.6
  classification_weight: 0.4
  temperature: 0.05

augmentation:
  enabled: true
  noise_std: 0.005
  rotation_angle: 3
  time_shift: 0.05
  scale_factor: 0.05

regularization:
  dropout: 0.3
  weight_decay: 0.01
  gradient_clip: 0.5
  early_stopping:
    enabled: true
    patience: 30
    min_delta: 0.0005

output:
  model_save_path: "outputs/clip_gesture_model_high_acc.pth"
  scaler_save_path: "outputs/clip_gesture_scaler_high_acc.pkl"
  info_save_path: "outputs/clip_gesture_info_high_acc.json"
  plots_save_path: "outputs/clip_training_curves_high_acc.png"
  confusion_matrix_path: "outputs/clip_confusion_matrix_high_acc.png"

experiment:
  name: "clip_gesture_high_accuracy"
  tags: ["clip", "gesture", "high_accuracy", "rocm"]
  notes: "High accuracy CLIP gesture recognition targeting 90%+"
